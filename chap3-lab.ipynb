{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, regexp_replace, flatten, explode, struct, create_map, array\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('chap_4').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import pandas as pd\n",
    "fake = Faker()\n",
    "def generate_data(num):\n",
    "    row = [{\"name\":fake.name(),\n",
    "           \"address\":fake.address(),\n",
    "           \"city\":fake.city(),\n",
    "           \"state\":fake.state(),\n",
    "           \"purchase_date\":fake.date_time(),\n",
    "            \"purchase_id\":fake.pyfloat(),\n",
    "             \"sales\":fake.pyfloat()\n",
    "           }]\n",
    "    return row\n",
    "panda = pd.DataFrame(generate_data(2))\n",
    "fake_data = spark.createDataFrame(panda)\n",
    "fake_data.write.format(\"parquet\").mode(\"append\").save(\"/Users/saisundarmasetty/Documents/data_architect_ws/chap3_lab_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bronze table from the dataset. The data should be streaming but set up to trigger once.\n",
    "location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap3_lab_data/\"\n",
    "format = \"parquet\"\n",
    "schema = spark.read.format(format).load(location).schema\n",
    "users = spark.readStream.schema(schema).format(format).load(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- purchase_date: timestamp (nullable = true)\n",
      " |-- purchase_id: double (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 19:24:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "bronze_schema = users.schema\n",
    "bronze_location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap4_lab_bronze/\"\n",
    "bronze_format = \"parquet\"\n",
    "checkpoint_location = f\"{bronze_location}/_checkpoint\"\n",
    "output_mode = \"append\"\n",
    "bronze_query = users.writeStream.format(bronze_format).trigger(once=True).option(\"checkpointLocation\", checkpoint_location).option(\"path\", bronze_location).outputMode(output_mode).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+---------------+----------+--------------------+-------------------+-------------------+\n",
      "|            name|             address|           city|     state|       purchase_date|        purchase_id|              sales|\n",
      "+----------------+--------------------+---------------+----------+--------------------+-------------------+-------------------+\n",
      "|   Sylvia Obrien|88043 William Mou...|  New Jaimestad|      Utah|1998-10-23 06:14:...| 3.4507044394328E13|2.53867269249356E11|\n",
      "|Katherine Fuller|5694 Reilly Mount...|Christopherside|Washington|1994-03-17 10:37:...|-8.38804796858555E9|   480.211623671276|\n",
      "|    David Nelson|4837 Wright Stati...| Christineburgh|   Vermont|1998-01-23 01:24:...|  -8152.74306742652| 8.2259285471756E12|\n",
      "+----------------+--------------------+---------------+----------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(bronze_format).schema(users.schema).load(bronze_location).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our silver table, we are going add the following columns:\n",
    "\n",
    "full_address: A combination of the address, city, and state\n",
    "Id: Round up/down purchase_id and find the absolute value\n",
    "Create and use a UDF to create a first_name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, lit, struct, concat, col, abs, floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap4_lab_bronze/\"\n",
    "schema = spark.read.format(\"parquet\").load(bronze_location).schema\n",
    "user_bronze = spark.readStream.format(\"parquet\").schema(schema).load(bronze_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the UDF function which deals with the datamanipulation\n",
    "@udf(returnType = StringType())\n",
    "def strip_name(x):\n",
    "    return x.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full address column , rounding up\n",
    "address_columns = [\"address\",\"city\",\"state\"]\n",
    "clean = user_bronze.select(col(\"*\"),concat(*address_columns).alias(\"full_address\"),floor(abs(\"purchase_id\")).alias(\"id\"),strip_name(\"name\").alias(\"first_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 19:24:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x128add130>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap4_lab_silver/\"\n",
    "silver_checkpoint = f\"{silver_location}/_checkpoint\"\n",
    "format = \"parquet\"\n",
    "output_mode = \"append\"\n",
    "clean.writeStream.format(format).option(\"checkpointLocation\",\"silver_checkpoint\").option(\"path\",\"silver_location\").trigger(once=True).outputMode(output_mode).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------------------------------------------+---------------+----------+--------------------------+-------------------+-------------------+--------------------------------------------------------------------------+--------------+----------+\n",
      "|name            |address                                                  |city           |state     |purchase_date             |purchase_id        |sales              |full_address                                                              |id            |first_name|\n",
      "+----------------+---------------------------------------------------------+---------------+----------+--------------------------+-------------------+-------------------+--------------------------------------------------------------------------+--------------+----------+\n",
      "|Sylvia Obrien   |88043 William Mountain Suite 486\\nEast Lynnland, LA 91295|New Jaimestad  |Utah      |1998-10-23 06:14:58.553893|3.4507044394328E13 |2.53867269249356E11|88043 William Mountain Suite 486\\nEast Lynnland, LA 91295New JaimestadUtah|34507044394328|Sylvia    |\n",
      "|Katherine Fuller|5694 Reilly Mountains\\nPort Erica, NM 17706              |Christopherside|Washington|1994-03-17 10:37:11.446394|-8.38804796858555E9|480.211623671276   |5694 Reilly Mountains\\nPort Erica, NM 17706ChristophersideWashington      |8388047968    |K         |\n",
      "|David Nelson    |4837 Wright Station\\nPort Sherri, NJ 46903               |Christineburgh |Vermont   |1998-01-23 01:24:17.207698|-8152.74306742652  |8.2259285471756E12 |4837 Wright Station\\nPort Sherri, NJ 46903ChristineburghVermont           |8152          |David     |\n",
      "+----------------+---------------------------------------------------------+---------------+----------+--------------------------+-------------------+-------------------+--------------------------------------------------------------------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\").load(\"silver_location\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gold Table : MAX, MIN sales and states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import lit, struct, sum,avg,max, min\n",
    "silver_location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap4_lab_bronze/\"\n",
    "schema = spark.read.format(\"parquet\").load(silver_location).schema\n",
    "users_silver = spark.readStream.format(\"parquet\").schema(schema).load(silver_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in ./chapter2/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: pyspark>=4.0.0 in ./chapter2/lib/python3.12/site-packages (from delta-spark) (4.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in ./chapter2/lib/python3.12/site-packages (from delta-spark) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./chapter2/lib/python3.12/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.23.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in ./chapter2/lib/python3.12/site-packages (from pyspark>=4.0.0->delta-spark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark and Delta Lake are configured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 19:24:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Create a SparkSession with Delta Lake support\n",
    "builder = SparkSession.builder.appName(\"DeltaApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"Spark and Delta Lake are configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1742.start.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version. SQLSTATE: 42K02\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:681)\n\tat org.apache.spark.sql.classic.DataStreamWriter.startInternal(DataStreamWriter.scala:247)\n\tat org.apache.spark.sql.classic.DataStreamWriter.start(DataStreamWriter.scala:136)\n\tat jdk.internal.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$6(DataSource.scala:665)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:665)\n\tat scala.util.Failure.orElse(Try.scala:230)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:665)\n\t... 13 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[160]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m gold_checkpoint_location = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgold_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/_checkpoint\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mformat\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdelta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mgold_agg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwriteStream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcheckpointLocation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mgold_checkpoint_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43monce\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mgold_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mappend\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data_architect_ws/chapter2/lib/python3.12/site-packages/pyspark/sql/streaming/readwriter.py:1704\u001b[39m, in \u001b[36mDataStreamWriter.start\u001b[39m\u001b[34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[39m\n\u001b[32m   1702\u001b[39m     \u001b[38;5;28mself\u001b[39m.queryName(queryName)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1706\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sq(\u001b[38;5;28mself\u001b[39m._jwrite.start(path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data_architect_ws/chapter2/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data_architect_ws/chapter2/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data_architect_ws/chapter2/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1742.start.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version. SQLSTATE: 42K02\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:681)\n\tat org.apache.spark.sql.classic.DataStreamWriter.startInternal(DataStreamWriter.scala:247)\n\tat org.apache.spark.sql.classic.DataStreamWriter.start(DataStreamWriter.scala:136)\n\tat jdk.internal.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$6(DataSource.scala:665)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:665)\n\tat scala.util.Failure.orElse(Try.scala:230)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:665)\n\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gold_agg = users_silver.groupBy(\"state\").agg(min(\"sales\").alias(\"minimum_sales\"),max(\"sales\").alias(\"maximum_sales\"),avg(\"sales\").alias(\"avg_sales\"))\n",
    "gold_location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap4_lab_gold/\"\n",
    "gold_checkpoint_location = f\"{gold_location}/_checkpoint\"\n",
    "format = \"delta\"\n",
    "gold_agg.writeStream.format(format).option(\"checkpointLocation\",gold_checkpoint_location).trigger(once=True).option(\"path\",gold_location).outputMode(\"append\").start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(format).load(gold_location).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
