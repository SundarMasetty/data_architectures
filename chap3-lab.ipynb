{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, regexp_replace, flatten, explode, struct, create_map, array\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType, TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('chap_4').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import pandas as pd\n",
    "fake = Faker()\n",
    "def generate_data(num):\n",
    "    row = [{\"name\":fake.name(),\n",
    "           \"address\":fake.address(),\n",
    "           \"city\":fake.city(),\n",
    "           \"state\":fake.state(),\n",
    "           \"purchase_date\":fake.date_time(),\n",
    "            \"purchase_id\":fake.pyfloat(),\n",
    "             \"sales\":fake.pyfloat()\n",
    "           }]\n",
    "    return row\n",
    "panda = pd.DataFrame(generate_data(2))\n",
    "fake_data = spark.createDataFrame(panda)\n",
    "fake_data.write.format(\"parquet\").mode(\"append\").save(\"/Users/saisundarmasetty/Documents/data_architect_ws/chap3_lab_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bronze table from the dataset. The data should be streaming but set up to trigger once.\n",
    "location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap3_lab_data/\"\n",
    "format = \"parquet\"\n",
    "schema = spark.read.format(format).load(location).schema\n",
    "users = spark.readStream.schema(schema).format(format).load(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- purchase_date: timestamp (nullable = true)\n",
      " |-- purchase_id: double (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 19:28:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "bronze_schema = users.schema\n",
    "bronze_location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap4_lab_bronze/\"\n",
    "bronze_format = \"parquet\"\n",
    "checkpoint_location = f\"{bronze_location}/_checkpoint\"\n",
    "output_mode = \"append\"\n",
    "bronze_query = users.writeStream.format(bronze_format).trigger(once=True).option(\"checkpointLocation\", checkpoint_location).option(\"path\", bronze_location).outputMode(output_mode).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+---------------+----------+--------------------+-------------------+-------------------+\n",
      "|            name|             address|           city|     state|       purchase_date|        purchase_id|              sales|\n",
      "+----------------+--------------------+---------------+----------+--------------------+-------------------+-------------------+\n",
      "|   Sylvia Obrien|88043 William Mou...|  New Jaimestad|      Utah|1998-10-23 06:14:...| 3.4507044394328E13|2.53867269249356E11|\n",
      "|Katherine Fuller|5694 Reilly Mount...|Christopherside|Washington|1994-03-17 10:37:...|-8.38804796858555E9|   480.211623671276|\n",
      "|    David Nelson|4837 Wright Stati...| Christineburgh|   Vermont|1998-01-23 01:24:...|  -8152.74306742652| 8.2259285471756E12|\n",
      "|    Corey Torres|3054 Christian Vi...|      Port Ryan|   Alabama|1988-11-07 03:16:...|   613647.515475998|  -34000.7887971917|\n",
      "+----------------+--------------------+---------------+----------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(bronze_format).schema(users.schema).load(bronze_location).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our silver table, we are going add the following columns:\n",
    "\n",
    "full_address: A combination of the address, city, and state\n",
    "Id: Round up/down purchase_id and find the absolute value\n",
    "Create and use a UDF to create a first_name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, lit, struct, concat, col, abs, floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap4_lab_bronze/\"\n",
    "schema = spark.read.format(\"parquet\").load(bronze_location).schema\n",
    "user_bronze = spark.readStream.format(\"parquet\").schema(schema).load(bronze_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the UDF function which deals with the datamanipulation\n",
    "@udf(returnType = StringType())\n",
    "def strip_name(x):\n",
    "    return x.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full address column , rounding up\n",
    "address_columns = [\"address\",\"city\",\"state\"]\n",
    "clean = user_bronze.select(col(\"*\"),concat(*address_columns).alias(\"full_address\"),floor(abs(\"purchase_id\")).alias(\"id\"),strip_name(\"name\").alias(\"first_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 19:28:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x10c966300>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap4_lab_silver/\"\n",
    "silver_checkpoint = f\"{silver_location}/_checkpoint\"\n",
    "format = \"parquet\"\n",
    "output_mode = \"append\"\n",
    "clean.writeStream.format(format).option(\"checkpointLocation\",\"silver_checkpoint\").option(\"path\",\"silver_location\").trigger(once=True).outputMode(output_mode).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------------------------------------------+---------------+----------+--------------------------+-------------------+-------------------+--------------------------------------------------------------------------+--------------+----------+\n",
      "|name            |address                                                  |city           |state     |purchase_date             |purchase_id        |sales              |full_address                                                              |id            |first_name|\n",
      "+----------------+---------------------------------------------------------+---------------+----------+--------------------------+-------------------+-------------------+--------------------------------------------------------------------------+--------------+----------+\n",
      "|Sylvia Obrien   |88043 William Mountain Suite 486\\nEast Lynnland, LA 91295|New Jaimestad  |Utah      |1998-10-23 06:14:58.553893|3.4507044394328E13 |2.53867269249356E11|88043 William Mountain Suite 486\\nEast Lynnland, LA 91295New JaimestadUtah|34507044394328|Sylvia    |\n",
      "|Katherine Fuller|5694 Reilly Mountains\\nPort Erica, NM 17706              |Christopherside|Washington|1994-03-17 10:37:11.446394|-8.38804796858555E9|480.211623671276   |5694 Reilly Mountains\\nPort Erica, NM 17706ChristophersideWashington      |8388047968    |K         |\n",
      "|David Nelson    |4837 Wright Station\\nPort Sherri, NJ 46903               |Christineburgh |Vermont   |1998-01-23 01:24:17.207698|-8152.74306742652  |8.2259285471756E12 |4837 Wright Station\\nPort Sherri, NJ 46903ChristineburghVermont           |8152          |David     |\n",
      "|Corey Torres    |3054 Christian Ville\\nLake Johnside, NM 93795            |Port Ryan      |Alabama   |1988-11-07 03:16:08.359282|613647.515475998   |-34000.7887971917  |3054 Christian Ville\\nLake Johnside, NM 93795Port RyanAlabama             |613647        |Corey     |\n",
      "+----------------+---------------------------------------------------------+---------------+----------+--------------------------+-------------------+-------------------+--------------------------------------------------------------------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\").load(\"silver_location\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gold Table : MAX, MIN sales and states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import lit, struct, sum,avg,max, min\n",
    "silver_location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap4_lab_bronze/\"\n",
    "schema = spark.read.format(\"parquet\").load(silver_location).schema\n",
    "users_silver = spark.readStream.format(\"parquet\").schema(schema).load(silver_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in ./chapter2/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: pyspark>=4.0.0 in ./chapter2/lib/python3.12/site-packages (from delta-spark) (4.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in ./chapter2/lib/python3.12/site-packages (from delta-spark) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./chapter2/lib/python3.12/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.23.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in ./chapter2/lib/python3.12/site-packages (from pyspark>=4.0.0->delta-spark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/02 19:27:48 WARN Utils: Your hostname, Sai-Sundar-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 10.0.0.78 instead (on interface en0)\n",
      "25/07/02 19:27:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/saisundarmasetty/Documents/data_architect_ws/chapter2/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/saisundarmasetty/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/saisundarmasetty/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ff7b50de-9061-4bc1-bddb-bbde6bebfd25;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.13;4.0.0!delta-spark_2.13.jar (461ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.0/delta-storage-4.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;4.0.0!delta-storage.jar (73ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.13.1/antlr4-runtime-4.13.1.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.13.1!antlr4-runtime.jar (78ms)\n",
      ":: resolution report :: resolve 1643ms :: artifacts dl 615ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ff7b50de-9061-4bc1-bddb-bbde6bebfd25\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (7933kB/7ms)\n",
      "25/07/02 19:27:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/02 19:27:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark and Delta Lake are configured.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Create a SparkSession with Delta Lake support\n",
    "builder = SparkSession.builder.appName(\"DeltaApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"Spark and Delta Lake are configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 19:28:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x10f831490>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 19:28:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gold_agg = users_silver.groupBy(\"state\").agg(min(\"sales\").alias(\"minimum_sales\"),max(\"sales\").alias(\"maximum_sales\"),avg(\"sales\").alias(\"avg_sales\"))\n",
    "gold_location = \"/Users/saisundarmasetty/Documents/data_architect_ws/chap4_lab_gold/\"\n",
    "gold_checkpoint_location = f\"{gold_location}/_checkpoint\"\n",
    "format = \"delta\"\n",
    "gold_agg.writeStream.format(format).option(\"checkpointLocation\",gold_checkpoint_location).trigger(once=True).option(\"path\",gold_location).outputMode(\"complete\").start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+\n",
      "|     state|      minimum_sales|      maximum_sales|          avg_sales|\n",
      "+----------+-------------------+-------------------+-------------------+\n",
      "|Washington|   480.211623671276|   480.211623671276|   480.211623671276|\n",
      "|   Alabama|  -34000.7887971917|  -34000.7887971917|  -34000.7887971917|\n",
      "|   Vermont| 8.2259285471756E12| 8.2259285471756E12| 8.2259285471756E12|\n",
      "|      Utah|2.53867269249356E11|2.53867269249356E11|2.53867269249356E11|\n",
      "+----------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(format).load(gold_location).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
